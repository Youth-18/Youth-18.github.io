---
layout:     post
title:      "论文理解-YOLOv2 "
date:       2018-8-16
author:     "Youth-18"
categories: 论文理解
tags:  论文理解
---  
  
参考：https://blog.csdn.net/jesse_mx/article/details/53925356
### 一、Better  
现在计算机视觉更倾向于更大、更深的网络。更好的表现通常是因为训练更大的网络和结合多种模型。而我们并没有扩大我们的网络，我们把从过去网络中得到的各种想法跟我们新的观念结合起来。结果如下图：
![](/blog_image/YOLOv20.png)
#### Batch Normalization  
添加batch normalization在mAP上获得了2%的提升，同时也会帮助规范网络，并且我们可以去掉dropout而不用担心过拟合。  
参考：https://blog.csdn.net/hjimce/article/details/50866313  
#### High Resolution Classifier  
yolov2输入图像的分辨率改为448\*448，在ImageNet数据集上训练10轮(10 epochs)，训练后的网络就可以适应高分辨率的输入了。然后，在检测过程中微调resulting network。这个高分辨率的输入在mAP上大约提升了4%。  
#### Convolutional With Anchor Boxes  
YOLO直接通过全卷积层直接预测bounding boxes的坐标，导致丢失较多的空间信息，定位不准。所以借鉴了Faster R-CNN中的anchor。作者去掉了全连接层，用anchor boxes来预测bounding boxes。首先去掉了一个pooling layer来使网络卷积层能输出更高的分辨率。我们用416\*416来代替输入图像的尺寸448\*448,目的是使后面的卷积特征图宽高为奇数，使有一个单独的center cell。因为物体特别是大物体，通常占据图像的中心，所以最好在中心有一个位置来预测这些对象，而不是四个相邻的位置。YOLO的卷积层把图片缩放了32倍，所以输入图片大小为416\*416时，输出的feature map的大小是13\*13。  
具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。  
#### Dimension Clusters  
在我们使用YOLO时会遇到两个问题，第一个问题就是box的维度是手动挑选的，虽然网络也会通过学习来调整网络的维度，但是在训练之前就得到更好的box，会使网络更好的预测。于是使用k-means来选择priors。当使用k-means时，用欧氏距离会使大的box比小的box产生更多的error。所以使用IOU距离，定义如下：
$$
d(box,centroid) = 1 - IOU(box,centroid)
$$
这样不会受到box大小的限制。![](/blog_image/YOLOv2_l0.png)  
运行k-means后得到5个box,不同于人工挑选的box，高瘦的box比矮胖的box多。并且表现跟9个anchor box相似。当得到9个box时，表现有大幅提升。
![](/blog_image/YOLOv2_l1.png)    
#### Direct location prediction  
第二个问题就是模型不稳定，尤其是在早期的迭代过程中。这种不稳定大多来自于预测box的(x,y)坐标。在区域建议网络上，预测$t_x,t_y$以及(x,y)坐标的公式如下：
$$
x = (t_x*w_a) - x_a \\
y = (t_y*h_a) - y_a
$$  
根据参考的博文，正确公式应该如下：  
$$
x = (t_x*w_a) + x_a \\
y = (t_y*h_a) + y_a
$$  
当t_x=1时，将把box向右移动anchor box宽度的距离。当t_x=-1，将box想左移动相同的距离。  
这个公式没有被限制，所以不管在什么位置进行预测，任意anchor box可以终止在图片的任一点。进行随机初始化后，模型会花费很长时间来稳定才能预测比较合理的位置。  
所以作者直接预测相对于grid cell位置的坐标。使用logistic激活函数来限制预测值的范围为0-1。  
在每个feature map上每一个cell预测5个bounding boxes，每个bounding box 预测5个值，$t_x,t_y,t_w,t_h,t_o$。$(c_x,x_y)$是cell相对于图片左上角的offset，bounding box的width和height为$p_w,p_h$，通过以下公式来预测：  
$$
b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y) + c_y \\
b_w = p_we^{t_w} \\
b_h = p_he^{t_h} \\
P_r(object)*IOU(b,object) = \sigma(t_o)
$$  
结合下图会更好理解，$t_x,t_y$是预测box相对于bounding box prior的offset，$c_x,c_y$是cell距离图片左上角的距离，使用e的幂函数是因为前面进行$ln$计算,$\sigma(t_x)$是bounding box的中心相对于栅格左上角的横坐标，$\sigma(t_y)$是纵坐标，$\sigma(t_o)$是confidence score。
![](/blog_image/YOLOv2_l2.png)   
#### Fine-Grained Features  
YOLO最后在13\*13的特征图上预测，所以对大物体检测较好，为了提高对小物体的检测精度，所以使用细粒度特征，Faster R-CNN和SSD都是在不同特征图上提取特征，而YOLO是把前面的特征叠加到后面的特征上，类似于ResNet上的identity mappings，只是通道的叠加而不是空间的叠加，将26\*26\*512的特征图采取隔行隔列的方式采样，得到四个13\*13\*512的特征图，叠加起来就是13\*13\*2048的特征图，然后接在最后的13\*13的特征图上。  
#### Multi-Scale Training  
原先YOLO输入图片的尺寸为448\*448，加入anchor box后调整为416\*416，因为模型仅仅使用了卷积跟池化，所以可以调整输入图片的大小。  
因此图片的下采样因子为32，即图片尺寸都为32的倍数，{320,352，...，608}。最小的为320\*320，最大的为608\*608，每10个bach我们的网络都会在这里面随机选择输入图片的尺寸进行训练。  
### 二、Faster  
舍弃VGG-16(分辨率为224\*224的图片一次正向传播需要306.9亿次浮点运算)而使用GoogLeNet(85.2亿)，精度有所下降，但下降不大(90.0%->88.0%)，速度提升。  
#### Darknet-19  
这个模型跟VGG模型相似之处是大量使用3\*3 filter，并在池化后把通道数翻倍。借鉴NIN，使用全局平均池化，并且使用1\*1 filter穿插在3\*3 filter之间用来压缩特征。使用BN来稳定训练，加速收敛，规范模型。  
Darknet-19有19个卷积层跟5个最大池化层，如下图：  
![](/blog_image/YOLOv2_l3.png)  
#### Training for classification  
给了训练所用参数，使用了random augmentation(random crops, rotations, hue, saturation, exposure shifts)。
#### Training for detection  
去除最后的卷积层，使用3个3\*3\*1024的卷积层，每个卷积层后面接一个1\*1的卷积，输出的数量是我们需要检测的数量。比如VOC中预测5个boxes，每个box预测5个坐标值跟20个类别，所以有125个filters。  
还添加了转移层，把最后的3\*3\*512层连接到倒数第一第二个卷积层，所以我们的模型可以处理细粒度特征。  
### 四、Stronger  
通过ImageNet训练分类，通过COCO和VOC训练检测，遇到分类集的图片只计算分类loss，遇到检测集的图片则计算所有loss。并建立Wordtree，如下图：
![](/blog_image/YOLOv2_l4.png)  
1）遍历Imagenet的label，然后在WordNet中寻找该label到根节点(指向一个物理对象)的路径；  
2）如果路径直有一条，那么就将该路径直接加入到分层树结构中；  
3）否则，从剩余的路径中选择一条最短路径，加入到分层树。  
并且使用了分组softmax,如下图：  
![](/blog_image/YOLOv2_l5.png)  

