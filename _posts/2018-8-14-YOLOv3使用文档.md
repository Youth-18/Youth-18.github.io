---
layout:     post
title:      "YOLOv3使用文档 "
date:       2018-8-14
author:     "Youth-18"
categories: 论文实践
tags:  论文实践
---
### 一、YOLOv3的配置  
参考：https://pjreddie.com/darknet/yolo/  
```
git clone https://github.com/pjreddie/darknet
cd darknet
```
修改Makefile文件：
```
GPU=1  #使用GPU
CUDNN=1  #使用CUDNN加速  
OPENCV=1 #使用opencv
```
```
make -j8
```
可测试一下
```
wget https://pjreddie.com/media/files/yolov3.weights
```
```
./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg
```

### 二、数据集的准备  
参考：https://blog.csdn.net/lilai619/article/details/79695109  
https://blog.csdn.net/john_bh/article/details/80625220  
#### 1.将自己的数据集转化为voc格式  
这个数据集的可放在任意位置，我是在根目录下建了一个data文件夹用来放置数据。
```
data
—— VOCdevkit
———— headdata
—————— Annotations
—————— ImageSets
———————— Main
—————— JPEGImages
```
名字可以更改，但后面的路径要写对。  
Annotations里面是.xml文件；  
Main中是4个txt文件，其中test.txt是测试集，train.txt是训练集，val.txt是验证集，trainval.txt是训练和验证集;  
JPEGImages中是所有的训练图片。
#### 2.生成YOLOv3使用的数据格式  
YOLOv3使用的是.txt数据格式，.txt里面如下图：
![](/blog_image/YOLOv3_txt.png)  
将darknet/scripts/voc_label.py复制到data目录下，跟VOCdevkit同目录。
修改里面的内容，参考我的[voc_label.py](https://github.com/Youth-18/tools/tree/master/YOLOv3)：
```python
{% highlight python linenos %}
import xml.etree.ElementTree as ET
import pickle
import os
from os import listdir, getcwd
from os.path import join

sets=[('train'), ('val'), ('test')]              #当你的Main下文件不需要year时，请去掉全文中所有year   

classes = ["person"]                              #修改为自己的类别


def convert(size, box):
    dw = 1./(size[0])
    dh = 1./(size[1])
    x = (box[0] + box[1])/2.0 - 1
    y = (box[2] + box[3])/2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)

def convert_annotation(image_id):
    in_file = open('VOCdevkit/headdata/Annotations/%s.xml'%(image_id))                            # 修改为自己.xml文件路径
    out_file = open('VOCdevkit/headdata/labels/%s.txt'%(image_id), 'w')                           # 修改label存放路径
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult)==1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')

wd = getcwd()

for image_set in sets:
    if not os.path.exists('VOCdevkit/headdata/labels/'):                                                   # 修改路径
        os.makedirs('VOCdevkit/headdata/labels/')
    image_ids = open('VOCdevkit/headdata/ImageSets/Main/%s.txt'%(image_set)).read().strip().split()        # 修改路径
    list_file = open('%s.txt'%(image_set), 'w')
    for image_id in image_ids:
        print(image_id)
        list_file.write('%s/VOCdevkit/headdata/JPEGImages/%s.jpg\n'%(wd, image_id))                        #修改路径
        convert_annotation(image_id)
    list_file.close()

os.system("cat train.txt val.txt > train1.txt")                                                            # 将文件合并为一个文件，最好文件名都不相同，生成后再把train1.txt改回train.txt
os.system("cat train.txt val.txt test.txt > train.all.txt")                                                # 
{% endhighlight %}
```
生成YOLOv3训练所用数据
```
python voc_label.py 
```
### 三、训练
#### 0.修改detector.c  
目的1：便于以后计算recall  
目的2：训练之前计算自己数据的anchor大小，从而更改cfg文件
```
./darknet detector calc_anchors cfg/voc.data -num_of_clusters 9 -width 416 -height 416 -show 1  
```
来聚类anchor,然后修改cfg/yolov3-train.cfg中的anchor，修改格式可参考我的[detector.c](https://github.com/Youth-18/tools/tree/master/YOLOv3)
#### 1.下载预训练模型  
```
wget https://pjreddie.com/media/files/darknet53.conv.74
```
#### 2.修改配置文件
（1）修改cfg/voc.data  
```
classes= 1                                              # 类别数，我的只有一类
train  = /home/data/datahead/train.txt       #生成的YOLOv3使用的train.txt
valid  = /home/data/datahead/test.txt        #生成的test.txt
names = /home/data/datahead/headdata.names   #自己建的.name，里面是具体类别
backup = /home/darknet/backup                #存储权重的文件夹

```
（2）修改.name文件  
一行放一个类别名  
（3）修改cfg/yolov3-voc.cfg
<font color=red>这里可以将yolov3-voc.cfg备份成两个文件，yolov3-train.cfg与yolov3-test.cfg,然后训练文件中只开启train,测试文件中只开启test，且将batch=1，subdivisions=1</font>
```
[net]
# Testing            ### 测试模式                                          
# batch=1
# subdivisions=1
# Training           ### 训练模式，每次前向的图片数目 = batch/subdivisions 
batch=64
subdivisions=16
width=416            ### 网络的输入宽、高、通道数
height=416
channels=3
momentum=0.9         ### 动量 
decay=0.0005         ### 权重衰减
angle=0
saturation = 1.5     ### 饱和度
exposure = 1.5       ### 曝光度 
hue=.1               ### 色调
learning_rate=0.001  ### 学习率 
burn_in=1000         ### 学习率控制的参数
max_batches = 50200  ### 迭代次数                                          
policy=steps         ### 学习率策略 
steps=40000,45000    ### 学习率变动步长 
scales=.1,.1         ### 学习率变动因子  



[convolutional]
batch_normalize=1    ### BN
filters=32           ### 卷积核数目
size=3               ### 卷积核尺寸
stride=1             ### 卷积核步长
pad=1                ### pad
activation=leaky     ### 激活函数

......

[convolutional]
size=1
stride=1
pad=1
filters=45  #3*(10+4+1)
activation=linear

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=10  #类别   
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=0  #1，如果显存很小，将random设置为0，关闭多尺度训练；
......

[convolutional]
size=1
stride=1
pad=1
filters=45  #3*(10+4+1)
activation=linear

[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=10  #类别
num=9
jitter=.3
ignore_thresh = .5
truth_thresh = 1
random=0  #1，如果显存很小，将random设置为0，关闭多尺度训练；
......

[convolutional]
size=1
stride=1
pad=1
filters=45  #3*(10+4+1)
activation=linear

[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes=10  #类别
num=9
jitter=.3  # 数据扩充的抖动操作
ignore_thresh = .5  #文章中的阈值1
truth_thresh = 1  #文章中的阈值2
random=0  #1，如果显存很小，将random设置为0，关闭多尺度训练；
```
必须要改的地方，classes的数目为你的类别数。然后根据你classes类别的数目修改最后一层卷积数，即[yolo]前面的一层卷积中filters。计算公式如下：
$$
N*N*[3*(4+1+C)]
$$
N=1，具体代表什么不确定。  
3，每组mask的数目，mask一共9个，分三组，每组三个，即每组预测的boxes的数量。    
4，预测的坐标，bounding box offsets。  
1，预测的置信度。  
C，类别数。  
所以类别数为10的时候，1\*1\*[3\*(4+1+10)]=45  
![](/blog_image/YOLOv3_cfg.png)  
#### 3.运行  
单GPU  
```
./darknet detector train cfg/voc.data cfg/yolov3-train.cfg darknet53.conv.74 | tee yolo.log   
```  
多GPU  
```
./darknet detector train cfg/voc.data cfg/yolov3-train.cfg darknet53.conv.74 -gpus 0,1,2,3 | tee yolo.log
```  
<font color=red>注意：一定要加 | tee {name}.log 这样才能保存运行过程中的各种数据，方便以后曲线可视化，比如loss跟IOU</font>  
当从某个已经保存的权重运行时：  
```
./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3-train_5000.weights -gpus 0,1,2,3
```  
当突然断掉，想从某个检查点运行时：  
```
./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3-loss train.backup -gpus 0,1,2,3
```    
#### 4.输出参数分析  
![](/blog_image/YOLOv3_t.png)
Region xx:cfg文件中mask所在layer；  
Avg IOU:当前迭代中，预测的box与groundtruth box的平均交并比；  
Class：标注物体的分类准确率，越大越好，期望数值为1；  
Obj:越大越好，期望数值为1；  
No Obj:越小越好，期望数值为0；  
.5R: 当IOU的阈值为0.5的时候，recall的大小；  
0.75R:当IOU的阈值为0.75的时候，recall的大小；
7634：第几个batch；  
1.077007：总损失；  
0.980247 avg：平均损失；  
0.001000 rate: 当前的学习率；  
7.004570 seconds: 这个batch的训练时间；  
488576 image：目前为止参与训练的图片总数。  
注：输出参数跟cfg文件定义的bach,subdivision有关，比如bach=64,subdivision=16，说明一个bach处理64个样本，又分16组，每组4个样本，每组又包含3个信息 
```
Region 82 Avg IOU: 
Region 94 Avg IOU: 
Region 106 Avg IOU:
```
### 四、测试
#### 1.loss曲线跟IOU曲线  
可参考[curve_visualization.py](https://github.com/Youth-18/tools/tree/master/YOLOv3)
#### 2.计算recall
```
./darknet detector recall cfg/voc.data cfg/yolov3-test.cfg results/yolov3-final.weights
```
#### 3.计算mAP  
首先将检测结果保存到result/person.txt。<font color=red>求哪一类的ap，就以类名命名。</font>
```
./darknet detector valid cfg/voc.data cfg/yolov3-test.cfg results/yolov3-final.weights -out person.txt -gpus 0,1 -thresh .5
```
下载py-faster-rcnn下的voc_eval.py，可使用我的[voc_eval.py](https://github.com/Youth-18/tools/tree/master/YOLOv3)    
新建compute_mAP.py
```python
from voc_eval import voc_eval
 
print voc_eval('./results/{}.txt', '/xxx/darknet/scripts/VOCdevkit/VOC2007/Annotations/{}.xml', '/xxx/darknet/scripts/VOCdevkit/VOC2007/ImageSets/Main/test.txt', 'person', '.')
```  
第一个参数是第一步生成的检测文件（{}.txt这个格式不要改,跟你第四个参数有关，为person.txt）；  
第二个参数是你数据集的标注文件（{}.xml这个格式不要改,跟你第五个参数有关，代表所有xml）；  
第三个参数是Main中的test.txt文件；  
第四个参数是第一步生成的文件的名字<font color=red>以自己的class命名，比如求person的ap那就是person.txt</font>；  
第五个参数是.，代表目录文件下所有文件。  
注：重复测试时，请删除./darknet/annots.pkl文件，执行python compute_mAP.py，返回的最后一个值就是AP。
